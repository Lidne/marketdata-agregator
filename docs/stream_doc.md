# Какой наилучший подход к записи данных из потока в postrgresql? Если делать insert при каждой новой записи, то у постгры не хватит мощности всё обработать, тк нагрузка большая (в районе тысячи сообщений в секунду). Как нужно выстроить модель обработки и сохранения данных в этом случае?

Для записи высокочастотных рыночных данных (~1000+ msg/sec) одиночные `INSERT` — это действительно антипаттерн, который "убьет" базу из-за оверхеда на транзакции и WAL (Write Ahead Log).

Наилучший подход — **микро-батчинг (Micro-batching) + `COPY` протокол**.

Ниже описана архитектура обработки от потока до БД.

### 1. Архитектурный паттерн: Buffer \& Batch

Вместо того чтобы писать каждое сообщение сразу, вы накапливаете их в буфере приложения и сбрасываете пачками.

**Схема:**
`Stream (gRPC/WebSocket) -> Application Memory Buffer -> Batch Worker -> PostgreSQL (COPY)`

#### Ключевые параметры батчинга:

1. **Size Limit (размер пачки):** например, 1000–5000 записей.
2. **Time Limit (таймаут):** например, 100–500 мс (если пачка не набралась, все равно сбрасываем, чтобы данные не протухали).

### 2. Использование протокола `COPY` (вместо INSERT)

Команда `COPY` в PostgreSQL предназначена для массовой загрузки. Она работает в **сотни раз быстрее** `INSERT`, так как:

- Минимизирует парсинг SQL (данные идут бинарным потоком или CSV).
- Выполняется в рамках одной транзакции.
- Оптимизирует запись в WAL.

**Пример на Go:**
Используйте интерфейс `CopyFrom` в драйвере `pgx`. Это стандарт де-факто для high-load в Go.

```go
// Псевдокод логики воркера
func (w *Worker) flushBatch(batch []Trade) error {
    // Начало транзакции
    tx, _ := db.Begin(ctx)
    defer tx.Rollback(ctx)

    // Используем pgx.CopyFrom
    rows := [][]interface{}{}
    for _, t := range batch {
        rows = append(rows, []interface{}{
            t.InstrumentUID, t.Side, t.Price, t.Quantity, t.TradedAt,
        })
    }

    _, err := tx.CopyFrom(
        ctx,
        pgx.Identifier{"trades"},
        []string{"instrument_uid", "side", "price", "quantity_lots", "traded_at"},
        pgx.CopyFromRows(rows),
    )

    return tx.Commit(ctx)
}
```

### 3. Дополнительные оптимизации TimescaleDB

Поскольку вы используете TimescaleDB, учтите следующее для настройки гипертаблиц:

1. **Chunk Time Interval:** Для 1000 msg/sec убедитесь, что размер чанка (chunk) влезает в оперативную память. Если чанки слишком мелкие или слишком большие, производительность вставки упадет. Для сделок интервал `1 day` или `1 week` обычно ок.
2. **Compression (Сжатие):** Включите нативную компрессию TimescaleDB для старых чанков (например, старше 1 дня).
   - Сжатые данные занимают в 10-20 раз меньше места.
   - **В сжатые чанки нельзя делать INSERT/UPDATE** (почти). Это идеально для исторических данных, но требует, чтобы данные приходили хронологически.
3. **Отключение лишних индексов:** При активной вставке каждый лишний индекс замедляет запись. Оставьте только `(instrument_uid, time)` и PK.

### Резюме стратегии

1. **Приложение:** Накапливает данные в памяти (slice/channel) до достижения `BatchSize` (2000 шт) или `BatchTimeout` (200 мс).
2. **Транспорт:** Использует бинарный протокол `COPY` (через `pgx.CopyFrom` в Go).
3. **Конкурентность:** Запускаете несколько воркеров (например, 2-4), которые пишут в БД параллельно. Не делайте слишком много соединений, Postgres этого не любит.
4. **Надежность:** Если батч упал с ошибкой, можно попробовать разбить его на части или записать поштучно (fallback), но чаще всего при ошибке сети просто делают retry всего батча.

Этот подход легко держит 10k-50k записей в секунду на одном инстансе Postgres средней мощности.
